{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_url(class_type_list):\n",
    "    \"\"\"Return the a dictionary containing class links for each\n",
    "    \n",
    "    Key: class type\n",
    "    Values: list of class links\n",
    "    \"\"\"\n",
    "    urls_dict = {}\n",
    "    url_template = 'https://www.skillshare.com/browse/%?sort=rating&seeAll=1'\n",
    "    for class_type in class_type_list:\n",
    "        urls_dict[class_type] = url_template.replace('%', class_type)\n",
    "    return urls_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_class_link(class_types, urls_dict):\n",
    "    for class_type in class_types:\n",
    "        i=0\n",
    "        print('Working on getting list of ' + class_type + ' classes')\n",
    "        url = urls_dict[class_type]\n",
    "        \n",
    "        driver = webdriver.Chrome(chromedriver)\n",
    "        driver.get(url)\n",
    "\n",
    "        try:\n",
    "            close = driver.find_element('xpath', \"//a[contains(@class,'btn-close ss-icon-close')]\")\n",
    "            close.click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        lastHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            i+=1\n",
    "            print(\"Load \", i)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(120)\n",
    "            newHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if newHeight == lastHeight:\n",
    "                break\n",
    "            lastHeight = newHeight\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        class_link_tags = soup.find_all('div', attrs={\"class\": \"col-4 class-column rendered \"})\n",
    "        class_links = [class_link_tag.find('p').find('a')['href'] for class_link_tag in class_link_tags]\n",
    "        class_links_dict[class_type] = class_links\n",
    "        driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_links_from_txt(file):\n",
    "    with open(file) as f:\n",
    "        urls_dict = {}\n",
    "        \n",
    "        reader = f.readlines()\n",
    "        \n",
    "        for line in reader:\n",
    "            isStart = True\n",
    "            foo = line.strip().split(',')\n",
    "            for item in foo:\n",
    "                clean_item = item.strip().strip(\"[\").strip(\"]\").strip(\"'\").strip()\n",
    "                if isStart:\n",
    "                    isStart = False\n",
    "                    class_type = clean_item\n",
    "                    urls_dict[class_type] = []\n",
    "                else:\n",
    "                    urls_dict[class_type].append(clean_item)\n",
    "        return urls_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get javascript that contains class info\n",
    "def get_javascript_data(soup):\n",
    "    projects_section = soup.find_all('script', attrs={\"type\": \"text/javascript\"})\n",
    "    \n",
    "    if not projects_section:\n",
    "        return None\n",
    "    \n",
    "    for js in projects_section:\n",
    "        try:\n",
    "            if js.string:\n",
    "                if re.search('.*SS.serverBootstrap =.*',js.text):\n",
    "                    javascript = js.string\n",
    "                    javascript = javascript.split(\"SS.serverBootstrap = \", 1)\n",
    "\n",
    "                    javascript_data = javascript[1].split(\";\\n        \")[0]\n",
    "                    javascript_data = json.loads(javascript_data)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    return javascript_data\n",
    "\n",
    "# get author link\n",
    "def get_title_and_author(soup):\n",
    "    try:\n",
    "        title = soup.find('title')\n",
    "        return [item.strip() for item in title.text.split('|')]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# get teacher detail info from the javascripte\n",
    "def get_detial_author_info(javascript_data):\n",
    "    try:\n",
    "        teacherInfo = javascript_data['pageData']['sectionData']['teacherInfo']\n",
    "        return teacherInfo['fullName'], teacherInfo['headline'], teacherInfo['profileUrl']\n",
    "    \n",
    "    except:\n",
    "        return None, None, None\n",
    "    \n",
    "# get class description info \n",
    "def get_class_description(soup):\n",
    "    try:\n",
    "        description_tag = (soup.find('div', attrs={\"class\": \"about-this-class\"})\n",
    "                           .find('div', attrs={\"class\": \"rich-content-wrapper\"}))\n",
    "\n",
    "        class_description = \"\"\n",
    "        images, hyperlinks = 0, 0\n",
    "        \n",
    "        for paragraph in description_tag.findChildren():\n",
    "            class_description += paragraph.text\n",
    "            if paragraph.name == 'img':\n",
    "                images += 1\n",
    "            elif paragraph.name == 'a':\n",
    "                hyperlinks += 1\n",
    "                \n",
    "        return len(class_description), images, hyperlinks\n",
    "    \n",
    "    except:\n",
    "        return None, None, None\n",
    "            \n",
    "# get class length\n",
    "def get_video_length(soup):\n",
    "    try:\n",
    "        video_content_tag = (soup.find('div', attrs={\"class\": \"summary\"})\n",
    "                             .text.strip().strip('\\n').strip())\n",
    "        return video_content_tag #video_num, video_length\n",
    "    except:\n",
    "        None\n",
    "\n",
    "\n",
    "# get tags linked to classes \n",
    "def get_tags(soup):\n",
    "    tags = []\n",
    "    try:\n",
    "        tags_section = soup.find('div', attrs={\"class\": \"tags-section\"})\n",
    "        for tag in tags_section.find_all('a'):\n",
    "            tags.append(tag.text.strip())\n",
    "    except:\n",
    "        pass\n",
    "    return tags\n",
    "        \n",
    "# get projects submitted for classes\n",
    "def get_project_authors(javascript_data):    \n",
    "    try:\n",
    "        return [project['author']['fullName'] \n",
    "                for project in javascript_data['pageData']['sectionData']['topProjects']]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# get class sku as index\n",
    "def get_class_sku(javascript_data):\n",
    "    try:\n",
    "        return javascript_data['classData']['sku']\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# identify if class is free\n",
    "def isPremium(javascript_data):\n",
    "    try:\n",
    "        return javascript_data['pageData']['headerData']['tagText'] == 'Premium class'\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# get class start date\n",
    "def get_start_date(javascript_data):\n",
    "    try:\n",
    "        return javascript_data['pageData']['syllabusData']['startTs']\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# get enrollment number\n",
    "def get_enrollment_number(javascript_data):\n",
    "    try:\n",
    "        return javascript_data['pageData']['sectionData']['numStudents']\n",
    "    return None\n",
    "\n",
    "def request_soup(url):\n",
    "    ua = UserAgent()\n",
    "    user_agent = {'User-agent': ua.random}\n",
    "    response  = requests.get(url, headers = user_agent)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = 'class_urls-1.csv'\n",
    "\n",
    "try:\n",
    "    urls_dict = get_links_from_txt(file)\n",
    "except:\n",
    "    urls_dict = make_url(class_types)\n",
    "    with open('class_list_urls.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for key, val in urls_dict.items():\n",
    "            writer.writerow([key, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['class_name',\n",
    "          'teacher',\n",
    "          'teacher_title',\n",
    "          'teacher_profile',\n",
    "          'description_length',\n",
    "          'description_image_number',\n",
    "          'description_link_number',\n",
    "          'class_length',\n",
    "          'tags',\n",
    "          'sample_project',\n",
    "          'class_sku',\n",
    "          'paid_class',\n",
    "          'start_date',\n",
    "          'enrollment_number']\n",
    "class_data = {col: [] for col in columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for class_type in class_types:\n",
    "    \n",
    "    print(\"working on \", class_type)\n",
    "    class_data = {col: [] for col in columns}\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for this_class in urls_dict[class_type]:        \n",
    "        time.sleep(5+2*random.random())\n",
    "        \n",
    "        try:\n",
    "            soup = request_soup(this_class)            \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        if not soup:\n",
    "            continue\n",
    "        \n",
    "        isSaved = False\n",
    "        while response.status_code != 200 or re.search('Access denied.*', soup.find('title').text):\n",
    "            print(\"stop here\")\n",
    "\n",
    "            if not isSaved:\n",
    "                pickle_title = class_type + \".pickle\"\n",
    "                pickle_out = open(pickle_title, \"wb\")\n",
    "                pickle.dump(class_data, pickle_out)\n",
    "                pickle_out.close()\n",
    "                isSaved = True\n",
    "                \n",
    "            time.sleep(60*5)\n",
    "            soup = request_soup(this_class) \n",
    "            \n",
    "        i += 1\n",
    "        if i%10 == 0:\n",
    "            print(i)\n",
    "\n",
    "        javascript_data = get_javascript_data(soup)\n",
    "\n",
    "        class_data['class_name'].append(get_title_and_author(soup)[0])\n",
    "\n",
    "        teacher, teacher_title, teacher_profile = get_detial_author_info(javascript_data)\n",
    "        class_data['teacher'].append(teacher)\n",
    "        class_data['teacher_title'].append(teacher_title)\n",
    "        class_data['teacher_profile'].append(teacher_profile)\n",
    "\n",
    "        description_length, description_image_number, description_link_number = get_class_description(soup)\n",
    "        class_data['description_length'].append(description_length)\n",
    "        class_data['description_image_number'].append(description_image_number)\n",
    "        class_data['description_link_number'].append(description_link_number)\n",
    "\n",
    "        class_data['class_length'].append(get_video_length(soup))\n",
    "\n",
    "        class_data['tags'].append(get_tags(soup))\n",
    "        class_data['sample_project'].append(teacher in get_project_authors(javascript_data))\n",
    "        class_data['class_sku'].append(get_class_sku(javascript_data))\n",
    "        class_data['paid_class'].append(isPremium(javascript_data))\n",
    "        class_data['start_date'].append(get_start_date(javascript_data))\n",
    "        class_data['enrollment_number'].append(get_enrollment_number(javascript_data))\n",
    "\n",
    "    pickle_title = class_type + \".pickle\"\n",
    "    pickle_out = open(pickle_title,\"wb\")\n",
    "    pickle.dump(class_data, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Appendix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_types = ['fine-art',\n",
    "               'photography',\n",
    "               'graphic-design',\n",
    "               'illustration',\n",
    "               'writing',\n",
    "               'music-production',\n",
    "               'animation',\n",
    "               'ui-ux-design',\n",
    "               'film-production',\n",
    "               'marketing',\n",
    "               'entrepreneurship',\n",
    "               'productivity',\n",
    "               'finance',\n",
    "               'freelance',\n",
    "               'business-analytics',\n",
    "               'management',\n",
    "               'leadership',\n",
    "               'sales',\n",
    "               'human-resources',\n",
    "               'accounting',\n",
    "               'web-development',\n",
    "               'mobile-development',\n",
    "               'it-security',\n",
    "               'data-science',\n",
    "               'game-design',\n",
    "               'product-management',\n",
    "               'crafts',\n",
    "               'culinary',\n",
    "               'health-and-wellness',\n",
    "               'other',\n",
    "               'teaching',\n",
    "               'home-business',\n",
    "               'languages',\n",
    "               'gaming']\n",
    "\n",
    "pickle.dump(class_types, open(\"class_types.pickle\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
